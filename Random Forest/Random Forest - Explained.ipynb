{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This blog will cover four parts:\n",
    "\n",
    "1. *What is random forest*\n",
    "2. *Why we want to use it*\n",
    "3. *How it works *\n",
    "4. *it's implementation in python sklearn*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What is random forest?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest is the advanced version of decision tree. It combines/ensembles numbers of decision trees for prediction, instead of just one. \n",
    "Note: If you don't know what decision tree is and are interested in the rationale behind it, please refer my previous blog on decision tree - https://medium.com/bite-sized-machine-learning/decision-tree-classifier-explained-9543dd052746\n",
    "\n",
    "![title](Capture0.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the benefit of using an ensemble method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you are not familiarize with the ensemble method/ensemble learning, you may ask why I need to use it.\n",
    "\n",
    "*Ensemble method*, in general, is a predictive model that make prediction based on a numbers of different models. The top three ensemble architectures are bagging, boosting and stacking. Random forest is an example of bagging, and that will be the focus for this blog.\n",
    "\n",
    "*Bagging* *is using random subsets of the training set to train the model*,  instead of using the entire training set.\n",
    "\n",
    "*Why we need ensemble method?* We can benefit from ensemble method because the individual model usually suffers the bias or variance. By combining different individual model, the ensemble model tends to more flexible (less bias) and less data-sensitive (less variance)\n",
    "\n",
    "* Bias is when the model not paying enough attention to the training data and not flexible enough to adapt along with the training data (e.g. model sticks with some wrong assumption)\n",
    "* Variance is when the model pay too much attention to the training data and captures a lot of noise of the data instead of the pattern\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does random forest work ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Random forest* is an ensemble model using *bagging *as the ensemble architecture and *decision tree* as individual model as weak learner. \n",
    "\n",
    "*Here are the four steps of building a random forest model:*\n",
    "\n",
    "![title](Capture1.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: ** select n (e.g. 1000) random subsets from the training set\n",
    "\n",
    "**Step 2: ** train n decision tree\n",
    "1.  one random subset is used for training one decision tree\n",
    "2. the optimal splits for each decision trees is based on a random subset of features (e.g. 10 feature in total, use a 5 random feature to split)\n",
    "\n",
    "**Step 3:** each tree predicts the records/candidates in the test set, independently. \n",
    "\n",
    "**Step 4:** select the majority vote from these 1000 tree on each record/candidate in the test set as the final decision \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation in python sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1: Create data set\n",
    "X, y = make_moons(n_samples=10000, noise=.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2: Split the training test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7555"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Fit a Decision Tree model as comparison\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7965"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Fit a Random Forest model, \" compared to \"Decision Tree model, accuracy go up by 5-6%\n",
    "clf = RandomForestClassifier(n_estimators=100,max_features=\"auto\",random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n_estimators **is how many tree you want to grow. In other word, how many subset you want to split and train\n",
    "\n",
    "**max_features **is the number of random features that individual decision tree will be used for finding the optimal splitting.\n",
    "\n",
    "* If “auto”, then max_features=sqrt(n_features).\n",
    "* If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).\n",
    "* If “log2”, then max_features=log2(n_features).\n",
    "* If None, then max_features=n_features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
